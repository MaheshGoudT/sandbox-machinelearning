{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo hostname -s 127.0.0.1\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/local/Cellar/apache-spark/2.4.0/libexec')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"testApp\").getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train spark.ml.lr model\n",
    "# https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "d = [{'name': 'A1', 'features': 60, 'lbl':1},\n",
    "     {'name': 'B1', 'features': 55, 'lbl':1},\n",
    "     {'name': 'A1', 'features': 30, 'lbl':0},\n",
    "     {'name': 'B1', 'features': 20, 'lbl':0},\n",
    "     {'name': 'A1', 'features': 10, 'lbl':0},\n",
    "     {'name': 'B1', 'features': 30, 'lbl':0},\n",
    "     {'name': 'A1', 'features': 25, 'lbl':0},\n",
    "     {'name': 'B1', 'features': 49, 'lbl':0}]\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"features\", IntegerType(), True),\n",
    "    StructField(\"lbl\", IntegerType(), True)\n",
    "    ])\n",
    "training = sqlContext.createDataFrame(sc.parallelize(d), schema)\n",
    "training = training.select(training['features'], training['lbl'])\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(training)\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "# mlrModel = mlr.fit(training)\n",
    "# print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "# print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the right threshold for spark.ml.lr model\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_col = ['features']\n",
    "lbl_col = 'lbl'\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df = pd.concat([df]*100).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTENC.html\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train_smote, y_train_smote = SMOTE(sampling_strategy=sampling_ratio, k_neighbors = k_neighbors, n_jobs = 4).fit_resample(X_train, y_train)\n",
    "\n",
    "brf = BalancedRandomForestClassifier(n_estimators=150, random_state=0, n_jobs=-1, verbose=1)\n",
    "brf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_brf = brf.predict(df[all_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train, y_train, test, y_test = training_df[numeric_col], training_df[lbl_col], testing_df[numeric_col], testing_df[lbl_col]\n",
    "\n",
    "def auc(m, train, test): \n",
    "    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]), metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n",
    "\n",
    "## Parameter Tuning\n",
    "# model = xgb.XGBClassifier()\n",
    "# param_dist = {\"max_depth\": [20,50],\n",
    "#               \"min_child_weight\" : [3,6],\n",
    "#               \"n_estimators\": [10,20],\n",
    "#               \"learning_rate\": [0.1,0.16],}\n",
    "# grid_search = GridSearchCV(model, param_grid=param_dist, cv = 2, verbose=10, n_jobs=-1)\n",
    "# grid_search.fit(train, y_train)\n",
    "# print(grid_search.best_estimator_)\n",
    "\n",
    "max_depth, min_child_weight, n_estimators, learning_r = 25, 1, 200, 0.2\n",
    "model = xgb.XGBClassifier(max_depth=max_depth, learning_rate=learning_r, min_child_weight=min_child_weight,  n_estimators=n_estimators, n_jobs=-1, verbose=1) \n",
    "model.fit(train,y_train)\n",
    "\n",
    "# Save model to file\n",
    "file_path = boosting_dataset_dir+\"maxdepth_{}_minchildweight_{}_estimators_{}_lr_{}.joblib\".format(max_depth,min_child_weight,n_estimators,learning_r) \n",
    "joblib.dump(model, file_path)\n",
    "## Load model\n",
    "# loaded_model = joblib.load(file_path)\n",
    "\n",
    "print(auc(model, train, test))\n",
    "\n",
    "predicted_probabilities = pd.Series(model.predict_proba(test)[:,1]).reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
