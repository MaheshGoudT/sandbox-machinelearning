{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!sudo hostname -s 127.0.0.1\n",
    "\n",
    "# import findspark\n",
    "# findspark.init('/usr/local/Cellar/apache-spark/2.4.0/libexec')\n",
    "\n",
    "# import pyspark\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# sc = pyspark.SparkContext(appName=\"testApp\").getOrCreate()\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Train spark.ml.lr model\n",
    "# # https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# d = [{'name': 'A1', 'features': 60, 'lbl':1},\n",
    "#      {'name': 'B1', 'features': 55, 'lbl':1},\n",
    "#      {'name': 'A1', 'features': 30, 'lbl':0},\n",
    "#      {'name': 'B1', 'features': 20, 'lbl':0},\n",
    "#      {'name': 'A1', 'features': 10, 'lbl':0},\n",
    "#      {'name': 'B1', 'features': 30, 'lbl':0},\n",
    "#      {'name': 'A1', 'features': 25, 'lbl':0},\n",
    "#      {'name': 'B1', 'features': 49, 'lbl':0}]\n",
    "# schema = StructType([\n",
    "#     StructField(\"name\", StringType(), True),\n",
    "#     StructField(\"features\", IntegerType(), True),\n",
    "#     StructField(\"lbl\", IntegerType(), True)\n",
    "#     ])\n",
    "# training = sqlContext.createDataFrame(sc.parallelize(d), schema)\n",
    "# training = training.select(training['features'], training['lbl'])\n",
    "# # training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "# lrModel = lr.fit(training)\n",
    "# print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "# print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# # mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "# # mlrModel = mlr.fit(training)\n",
    "# # print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "# # print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Choose the right threshold for spark.ml.lr model\n",
    "\n",
    "# trainingSummary = lrModel.summary\n",
    "\n",
    "# # Obtain the objective per iteration\n",
    "# objectiveHistory = trainingSummary.objectiveHistory\n",
    "# print(\"objectiveHistory:\")\n",
    "# for objective in objectiveHistory:\n",
    "#     print(objective)\n",
    "\n",
    "# # Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "# trainingSummary.roc.show()\n",
    "# print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# # Set the model threshold to maximize F-Measure\n",
    "# fMeasure = trainingSummary.fMeasureByThreshold\n",
    "# maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "# bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "#     .select('threshold').head()['threshold']\n",
    "# lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create pandas train/test df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# all_col = ['features']\n",
    "# lbl_col = 'lbl'\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(3,3))\n",
    "df.columns = ['n1','n2','n3']\n",
    "df['c1'] = ['cat','bat','go']\n",
    "df['c2'] = ['cats','bats','gos']\n",
    "df['label'] = [1,1,0]\n",
    "\n",
    "#replicate df and add noise to df\n",
    "df = pd.concat([df]*50, ignore_index=True)\n",
    "noise = np.random.normal(0, 1, (df.shape[0],3))\n",
    "df[['n1','n2','n3']] = df[['n1','n2','n3']] + noise\n",
    "\n",
    "#Assign some dummy train/test variable names\n",
    "all_col = ['n1','n2','n3']\n",
    "# g_train, g_y_train, X_test, y_test = df[all_col].values, df[['label']].values, df[all_col].values, df[['label']].values\n",
    "g_train, g_y_train, X_test, y_test = df[all_col], df[['label']], df[all_col], df[['label']]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(g_train, g_y_train, train_size=.8, random_state=0)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "\n",
    "params = {}\n",
    "x, y = X_train, y_train\n",
    "validate = True\n",
    "\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "# Set STDERR handler as the only handler \n",
    "log.handlers = [handler]\n",
    "\n",
    "local_params = {\n",
    "    'num_leaves': int(params.get('clf.num_leaves', '4096')),\n",
    "    'max_depth': int(params.get('clf.max_depth', '20')),\n",
    "    'num_iterations': int(params.get('clf.num_iterations', '200')),\n",
    "    'metric': ['l1', 'l2'],\n",
    "    'learning_rate': 0.1,\n",
    "    'min_sum_hessian_in_leaf': 100,\n",
    "    'min_data_in_leaf': 1,\n",
    "    'is_unbalance': 'true',\n",
    "    #'zero_as_missing': 'true' #There is slight increase in recall(0.2%) leaving null as is with l1/l2 metric\n",
    "    'verbose_eval': int(params.get('clf.verbose', '0'))\n",
    "}\n",
    "\n",
    "time_zero = time.time()\n",
    "log.info('train')\n",
    "evals_result = {}\n",
    "lgb_train = lgb.Dataset(x, y)\n",
    "clf = lgb.train(local_params,\n",
    "                  lgb_train,\n",
    "                  valid_sets=[],\n",
    "                  evals_result=evals_result)\n",
    "log.info('training done')\n",
    "time.sleep(2)\n",
    "duration = '%.4f min'%((time.time() - time_zero)/60.0)\n",
    "\n",
    "model_metrics = {'model_type':'lgbm', 'train_time':duration, 'model_params':json.dumps(local_params)}\n",
    "if validate:\n",
    "    #Inspect insample predictions spread on training dataset\n",
    "    y_train_prediction = clf.predict(x)\n",
    "    insample_prediction_spread = pd.DataFrame(y_train_prediction).describe()[0].to_dict()\n",
    "    for k in insample_prediction_spread:\n",
    "        val = '%.4f'%(insample_prediction_spread[k])\n",
    "        key = 'v_isps_' + str(k) #v_isps -> validation insample prediction spread\n",
    "        model_metrics[key] = val\n",
    "log.info(model_metrics)\n",
    "\n",
    "# ##Inspect insample corr values\n",
    "# #import scipy #scipy.stats.spearmanr\n",
    "# y_train_prediction = clf.predict(X_train)\n",
    "# np.corrcoef(y_train_prediction, np.array(y_train['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTENC.html\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# sampling_ratio, k_neighbors = 0.1, 5\n",
    "# X_train_smote, y_train_smote = SMOTE(sampling_strategy=sampling_ratio, k_neighbors = k_neighbors, n_jobs = -1).fit_resample(X_train, y_train)\n",
    "X_train_smote, y_train_smote = X_train, y_train\n",
    "\n",
    "brf = BalancedRandomForestClassifier(n_estimators=150, random_state=0, n_jobs=-1, verbose=1)\n",
    "brf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_brf = brf.predict(df[all_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aucpr(ground_truth, predictions):\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(ground_truth, predictions)\n",
    "    area = sklearn.metrics.auc(recall, precision) #Is this same as AP ?\n",
    "    return area\n",
    "    #print ('AP', sklearn.metrics.average_precision_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "def get_aucroc(m, train, test, y_train, y_test): \n",
    "    train_auc, test_auc = sklearn.metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]), sklearn.metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]) \n",
    "    return test_auc\n",
    "\n",
    "def get_training_validation_datasets_from_training_dataset(cur_df, tr_percentage=0.9):\n",
    "    atr_samples = cur_df.shape[0]\n",
    "    tr_st_idx, tr_end_idx = 0, int(tr_samples*tr_percentage)\n",
    "    va_st_idx, va_end_idx = tr_end_idx+1, tr_samples-1\n",
    "    tmp_training_df, tmp_validation_df = cur_df.iloc[tr_st_idx:tr_end_idx+1], cur_df.iloc[va_st_idx:va_end_idx+1]\n",
    "    return tmp_training_df, tmp_validation_df\n",
    "\n",
    "# tmp_training_df, tmp_validation_df = get_training_validation_datasets_from_training_dataset(training_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyperopt based parameter tuned version of model\n",
    "# https://www.kaggle.com/dreeux/hyperparameter-tuning-using-hyperopt\n",
    "# https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/\n",
    "# https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.fmin import fmin\n",
    "import xgboost as xgb\n",
    "\n",
    "def objective(space):\n",
    "    clf = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                            n_estimators = space['n_estimators'],\n",
    "                            max_depth = space['max_depth'],\n",
    "                            learning_rate = space['learning_rate'],\n",
    "                            n_jobs=-1\n",
    "                           )\n",
    "    eval_set  = [(train, y_train), (valid, y_valid)]\n",
    "    clf.fit(train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric = 'aucpr', early_stopping_rounds=5)\n",
    "    pred = clf.predict_proba(valid)[:,1]\n",
    "    aucpr = get_aucpr(y_valid.values, pred)\n",
    "    print (\"SCORE:\", aucpr)\n",
    "    return{'loss':aucpr, 'status':STATUS_OK}\n",
    "\n",
    "space = {\n",
    "        'max_depth': hp.choice('max_depth', np.arange(5, 25, dtype=int)),\n",
    "        'n_estimators' : hp.choice('n_estimators', np.arange(100, 500, 100, dtype=int)),\n",
    "        'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n",
    "        'scale_pos_weight' : hp.choice('scale_pos_weight',[1,2,5,10,50,100,1000])\n",
    "    }\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=1,\n",
    "            trials=trials)\n",
    "\n",
    "trials_file = boosting_dataset_dir+'hyperopt_trials.bin'\n",
    "pickle.dump(trials, open(trials_file, \"wb\"))\n",
    "# trials = pickle.load(open(trails_file, \"rb\"))\n",
    "\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -conda install py-xgboost\n",
    "# -conda install py-xgboost-gpu\n",
    "# -pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Parameter Tuning\n",
    "# model = xgb.XGBClassifier()\n",
    "# param_dist = {\"max_depth\": [20,50],\n",
    "#               \"min_child_weight\" : [3,6],\n",
    "#               \"n_estimators\": [10,20],\n",
    "#               \"learning_rate\": [0.1,0.16],}\n",
    "# grid_search = GridSearchCV(model, param_grid=param_dist, cv = 2, verbose=10, n_jobs=-1)\n",
    "# grid_search.fit(train, y_train)\n",
    "# print(grid_search.best_estimator_)\n",
    "\n",
    "## https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n",
    "max_depth, min_child_weight, n_estimators, learning_r = 6, 1, 200, 0.2\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', max_depth=max_depth, learning_rate=learning_r, min_child_weight=min_child_weight, n_estimators=n_estimators, n_jobs=-1, verbose=2) \n",
    "\n",
    "# dtrain = xgb.DMatrix(train, label=y_train)\n",
    "model.fit(X_train,y_train.ravel(),eval_metric=['aucpr'],eval_set=[(X_val,y_val.ravel())],early_stopping_rounds=8)\n",
    "\n",
    "# ## Save model to file\n",
    "# file_path = boosting_dataset_dir+\"maxdepth_{}_minchildweight_{}_estimators_{}_lr_{}.joblib\".format(max_depth,min_child_weight,n_estimators,learning_r) \n",
    "# joblib.dump(model, file_path)\n",
    "# ## Load model\n",
    "# # loaded_model = joblib.load(file_path)\n",
    "\n",
    "print(auc(model, X_train, X_test))\n",
    "predicted_probabilities = pd.Series(model.predict_proba(X_test)[:,1]).reset_index(drop=True)\n",
    "print(predicted_probabilities.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Importance\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "# xgb.plot_tree(model, num_trees=2)\n",
    "# xgb.to_graphviz(model, num_trees=2)\n",
    "\n",
    "# model.feature_names = xgtrain.feature_names\n",
    "tree_pdf_dir = '/Users/maheshgoud/Documents/Data/fraud_data/Results/trees'\n",
    "for tree_index in range(1,2):\n",
    "    dot = xgb.to_graphviz(model, num_trees=tree_index)\n",
    "    dot.render(\"{}/tree{}\".format(tree_pdf_dir,tree_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/slundberg/shap\n",
    "import shap\n",
    "# shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
    "# shap.force_plot(explainer.expected_value, shap_values[0,:], train[0,:])\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "shap.dependence_plot(\"Feature 0\", shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
