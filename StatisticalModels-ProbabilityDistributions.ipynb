{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow, PyTorch, PyMC3 Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import scipy.optimize as so\n",
    "from scipy.optimize import minimize, fmin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tf.contrib.distributions\n",
    "# tfd = tfp.distributions\n",
    "# tfb = tfp.distributions.bijectors\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "##Helpful links for Distributions\n",
    "# https://arxiv.org/pdf/1711.10604.pdf\n",
    "# https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Understanding%20TensorFlow%20Distributions%20Shapes.ipynb\n",
    "# https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html\n",
    "# https://www.tensorflow.org/api_docs/python/tf/distributions/Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tinker with tf distributions\n",
    "\n",
    "dist = tf.distributions.Normal(loc=0., scale=3.)\n",
    "print(dist.cdf(0.))\n",
    "dist = tf.distributions.Normal(loc=[1, 2.], scale=[11, 22.])\n",
    "# Evaluate the pdf of the first distribution on 0, and the second on 1.5, returning a length two tensor \n",
    "print(dist.prob([0, 1.5]))\n",
    "\n",
    "# Get 3 samples, returning a 3 x 2 tensor.\n",
    "print(dist.sample([3]))\n",
    "print(dist.sample([3]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Tinker with pytorch distributions\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "m = Bernoulli(torch.Tensor([0.3]))\n",
    "r = []\n",
    "for i in range(20):\n",
    "    r.append(m.sample().numpy()[0])\n",
    "print(r)  # 30% chance 1; 70% chance 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tinker with tf mixture of distributions\n",
    "# https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Mixture\n",
    "\n",
    "mixture_components = [1.0/2]*2\n",
    "bimix_gauss = tfd.Mixture(\n",
    "  cat=tfd.Categorical(probs=mixture_components),\n",
    "  components=[\n",
    "    tfd.Normal(loc=-1., scale=0.1),\n",
    "    tfd.Normal(loc=+1., scale=0.5)\n",
    "])\n",
    "\n",
    "#Is Weibull bijector the only way to directly use weibull distribution in tf ?\n",
    "\n",
    "x = tf.linspace(-2., 3., int(1e4))\n",
    "bimix_gauss.prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Tinker with PyMC distributions. Wrap scipy distribution in PyMC distribution\n",
    "\n",
    "y = pm.Binomial.dist(n=10, p=0.5)\n",
    "print(y.logp(5).eval(), y.logp(4).eval(), y.logp(6).eval())\n",
    "print(y.logp_sum(5).eval(), y.logp_sum(4).eval(), y.logp_sum(6).eval())\n",
    "print(y.random(size=10))\n",
    "\n",
    "with pm.Model() as model:\n",
    "    g = pm.Gamma('g', 1, 1)\n",
    "print(model.vars, model.deterministics)\n",
    "\n",
    "#Invoke scipy dist through PyMC \n",
    "print(ss.binom(10, 0.3).cdf(5))\n",
    "\n",
    "import theano\n",
    "with pm.Model() as model:\n",
    "    g = pm.Gamma('g', 1, 1)\n",
    "    b =  ss.binom(10, 0.3)\n",
    "    z = b + g\n",
    "    tmp_trace = pm.sample(1000)\n",
    "\n",
    "pm.summary(tmp_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Distributions / Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.binomial(10, 0.3, 10))\n",
    "print(np.random.binomial(10, 0.3, 10).mean())\n",
    "\n",
    "print(ss.binom(10, 0.3).rvs(10))\n",
    "print(ss.binom(10, 0.3).cdf(5))\n",
    "print(ss.binom(10, 0.3).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using scipy optimize with custom distributions\n",
    "# https://www.projectrhea.org/rhea/index.php/MLE_Examples:_Exponential_and_Geometric_Distributions_Old_Kiwi\n",
    "\n",
    "from scipy.stats import geom, gamma\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def neg_loglike(theta):\n",
    "    prob = lambda x, y, z: -x * np.log(geom.pmf(y, z))\n",
    "    data = [10, 9, 8, 7, 6, 5]\n",
    "    return np.sum([prob(data[i], i+1, theta) for i in range(len(data))])\n",
    "\n",
    "theta_start = 0.5\n",
    "res = minimize(neg_loglike, theta_start, method = 'Nelder-Mead', options={'disp':True})\n",
    "print(res)\n",
    "\n",
    "print('------------')\n",
    "\n",
    "def weibull_cdf(t, l, c):\n",
    "    return 1 - math.exp(-l * (t**c))\n",
    "\n",
    "def weibull_2seg_2clust_withspike_negloglike_dummy(theta): #TODO: Implement correct function later\n",
    "    prob = lambda x, y, z: -x * np.log(gamma.pdf(y, z))\n",
    "    data = [10, 9, 8, 7, 6, 5]\n",
    "    return np.sum([prob(data[i], i+1, theta) for i in range(len(data))])\n",
    "\n",
    "param_start = 0.5\n",
    "res = minimize(weibull_2seg_2clust_withspike_negloglike_dummy, param_start, method = 'Nelder-Mead', options={'disp':True}) \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/scipy/scipy/issues/3056\n",
    "\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "X = np.array([[1.020626, 1.013055], [0.989094, 1.059343]])\n",
    "freq = 13.574380165289256\n",
    "x_0 = [1., 1.]\n",
    "\n",
    "def objective(b):\n",
    "    def foo(r_log, freq):\n",
    "        mu, sd = r_log.mean(), r_log.std()\n",
    "        sd += 0.5 / freq\n",
    "        return mu / sd * np.sqrt(freq)\n",
    "\n",
    "    print(b)\n",
    "    return -foo(np.log(np.maximum(np.dot(X - 1, b) + 1, 0.2)), freq=freq)\n",
    "\n",
    "cons = ({'type': 'ineq', 'fun': lambda b: 2. - sum(b)},)\n",
    "res = optimize.minimize(objective, x_0, bounds=[(0., 2.)]*len(x_0), constraints=cons, method='slsqp')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Poisson Regression\n",
    "# https://stackoverflow.com/questions/47686227/poisson-regression-in-statsmodels-and-r\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.formula.api\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.families import Poisson\n",
    "\n",
    "df = pd.DataFrame(np.random.randint(100, size=(50,2)))\n",
    "df.rename(columns={0:'X1', 1:'X2'}, inplace=True)\n",
    "\n",
    "# glm = statsmodels.formula.api.gee\n",
    "# model = glm(\"X2 ~ X1\", groups=None, data=df, family=Poisson())\n",
    "# results = model.fit()\n",
    "# print(results.summary())\n",
    "\n",
    "# Add a column of ones for the intercept to create input X. This model is similar to R implementation\n",
    "X = np.column_stack( (np.ones((df.shape[0], 1)), df.X1) )\n",
    "y = df.X2\n",
    "print(sm.GLM(y, X, family = Poisson()).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Try implementing following papers using scipy, octave, tensorflow, pytorch (whichever is flexbile, scalable)\n",
    "#https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1048&context=wharton_research_scholars\n",
    "#https://pdfs.semanticscholar.org/d566/7b80c85b221aedf9c498fdd1d98a4478118b.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy data\n",
    "ts_data1 = np.array([8,7,6,5,4,3,3,3,3,4,5,6,7,8,9,10])\n",
    "ts_data2 = np.concatenate((5+ts_data1[0:8], ts_data1[8:]))\n",
    "ts_data3 = np.concatenate((ts_data1[0:8], 5+ts_data1[8:]))\n",
    "ts_data = np.array([ts_data1.cumsum(), ts_data2.cumsum(), ts_data3.cumsum()])\n",
    "ts_data = np.tile(ts_data,(5,1))\n",
    "ts_data_sum = np.sum(ts_data, axis=1)\n",
    "\n",
    "# ts_data_context = np.array(['a','b','b','b','b','c','c','c','c','c','d','d','d','d','d','e'])\n",
    "print(len(ts_data), ts_data_sum) #,len(ts_data_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "tot_num_clusters = 2 #num_clusters + 1\n",
    "num_samples = len(ts_data)\n",
    "\n",
    "clust_assign = np.random.randint(num_clusters, size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1.020626, 1.013055], [0.989094, 1.059343]])\n",
    "freq = 13.574380165289256\n",
    "x_0 = [1., 1.]\n",
    "\n",
    "def objective(b):\n",
    "    def foo(r_log, freq):\n",
    "        mu, sd = r_log.mean(), r_log.std()\n",
    "        sd += 0.5 / freq\n",
    "        return mu / sd * np.sqrt(freq)\n",
    "\n",
    "    print(b)\n",
    "    return -foo(np.log(np.maximum(np.dot(X - 1, b) + 1, 0.2)), freq=freq)\n",
    "\n",
    "cons = ({'type': 'ineq', 'fun': lambda b: 2. - sum(b)},)\n",
    "res = so.minimize(objective, x_0, bounds=[(0., 2.)]*len(x_0), constraints=cons, method='slsqp')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/pymc-devs/pymc3/blob/master/pymc3/distributions/mixture.py\n",
    "# https://docs.pymc.io/notebooks/model_comparison.html#Hierarchical-model\n",
    "# https://github.com/pymc-devs/pymc3/tree/master/pymc3/examples\n",
    "# http://people.duke.edu/~ccc14/sta-663-2016/16C_PyMC3.html\n",
    "\n",
    "# with pm.Model() as weibull_2seg_2clust_withspike: #TODO: Has bugs fix it later\n",
    "#     gamma_params = pm.Uniform('gamma_params', lower=0.0, upper=30.0, shape=(2,)) #alpha, beta\n",
    "#     shape = pm.Uniform('shape', lower=0.0, upper=30.0)\n",
    "#     scale_arr = pm.Gamma('scale_arr', alpha=gamma_params[0], beta=gamma_params[1], shape=(2,))\n",
    "#     weibull = pm.Weibull('weibull', alpha=shape, beta=scale_arr, shape=(2,))\n",
    "    \n",
    "#     #obs = pm.Normal('obs', mu=10 * weibull[0] + 10 * weibull[1], sd=10, observed=ts_data[0][0:6]) \n",
    "#     #trace = pm.sample(1000) #pm.sample(niter, step=step, start=start, njobs=4, random_seed=123) \n",
    "    \n",
    "#     # As we just need the logp, rather than add a RV to the model, we need to call .dist()\n",
    "#     #components = pm.Poisson.dist(mu=lam, shape=(2,))  \n",
    "#     w = pm.Dirichlet('w', a=np.array([1, 1]))\n",
    "#     like = pm.Mixture('like', w=w, comp_dists=weibull, observed=ts_data[0])\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "#Use iterable of distributions instead of array of random variables\n",
    "with pm.Model() as weibull_2seg_2clust_withspike:\n",
    "    gamma_alpha = pm.Uniform('gamma_alpha', lower=0.0, upper=30.0)\n",
    "    gamma_beta = pm.Uniform('gamma_beta', lower=0.0, upper=30.0)\n",
    "    shape1 = pm.HalfNormal('shape1', 1.)\n",
    "    scale1 = pm.Gamma('scale1', alpha=gamma_alpha, beta=gamma_beta)\n",
    "    shape2 = pm.HalfNormal('shape2', 1.)\n",
    "    scale2 = pm.Gamma('scale2', alpha=gamma_alpha, beta=gamma_beta)   \n",
    "    #weibull1 = pm.Weibull('weibull1', alpha=shape1, beta=scale1)\n",
    "    #weibull2 = pm.Weibull('weibull2', alpha=shape2, beta=scale2)   \n",
    "    #obs = pm.Normal('obs', mu=weibull1 + weibull2, sd=10, observed=[1,2,3,4,5])\n",
    "    weibull1 = pm.Weibull.dist(alpha=shape1, beta=scale1)\n",
    "    weibull2 = pm.Weibull.dist(alpha=shape2, beta=scale2)\n",
    "    final_cluster_prob = pm.Uniform.dist(lower=0.0, upper=1.0)\n",
    "    \n",
    "    w1 = pm.Dirichlet('w1', a=np.array([1., 1., 1.]))\n",
    "    obs1 = pm.Mixture('obs1', w=w1, comp_dists=[weibull1, weibull2, final_cluster_prob], observed=[1,2,3,4,5])\n",
    "    w2 = pm.Dirichlet('w2', a=np.array([1., 1., 1.]))\n",
    "    obs2 = pm.Mixture('obs2', w=w2, comp_dists=[weibull1, weibull2, final_cluster_prob], observed=[6,7,8,9,10])\n",
    "    \n",
    "    trace = pm.sample(1000)\n",
    "    \n",
    "    #inference = pm.fit(method='advi')  #This is mean field ADVI. Something like 'fullrank_advi' might be tried too \n",
    "    #trace = inference.sample(1000)\n",
    "    \n",
    "    #start = pm.find_MAP() # Find good starting point\n",
    "    #step = pm.Slice() # Instantiate MCMC sampling algorithm\n",
    "    #trace = pm.sample(1000, step, start=start, progressbar=True) # draw posterior samples using slice sampling     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Tinker with custom Weibull Gamma distribution in PyMC\n",
    "# def logp(failure, value):\n",
    "#     return (failure * log(λ) - λ * value).sum()\n",
    "\n",
    "# exp_surv = pm.DensityDist('exp_surv', logp, observed={'failure':failure, 'value':t})\n",
    "\n",
    "# Weibull fit to data\n",
    "with pm.Model():\n",
    "    data = [2,6,5,4,3] #[3,4,5,6,2]\n",
    "    alpha = pm.HalfNormal('alpha')\n",
    "    beta = pm.HalfNormal('beta')\n",
    "    like = pm.Weibull('observation', alpha, beta, observed=data)\n",
    "    start = pm.find_MAP()\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000, step, start=start)\n",
    "\n",
    "display(pm.summary(trace))\n",
    "pm.traceplot(trace), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with pm.Model() as model:\n",
    "#     lam = pm.Exponential('lam', lam=1, shape=(2,))  # `shape=(2,)` indicates two mixtures.\n",
    "#     # As we just need the logp, rather than add a RV to the model, we need to call .dist()\n",
    "#     components = pm.Poisson.dist(mu=lam, shape=(2,))\n",
    "#     w = pm.Dirichlet('w', a=np.array([1, 1]))  # two mixture component weights.\n",
    "#     like = pm.Mixture('like', w=w, comp_dists=components, observed=[1,2,3,6,4,3,2])\n",
    "#     trace = pm.sample(1000)\n",
    "\n",
    "## Mixture example\n",
    "# with pm.Model() as model:\n",
    "#     lam = pm.Exponential('lam', lam=1, shape=(2,))  # `shape=(2,)` indicates two mixtures.\n",
    "#     # As we just need the logp, rather than add a RV to the model, we need to call .dist()\n",
    "#     components = pm.Poisson.dist(mu=lam, shape=(2,))\n",
    "#     w = pm.Dirichlet('w', a=np.array([1, 1]))  # two mixture component weights.\n",
    "#     like = pm.Mixture('like', w=w, comp_dists=components, observed=[1,2,3,6,4,3,2])\n",
    "#     trace = pm.sample(1000)\n",
    "\n",
    "## Compute MAP estimate\n",
    "# map_estimate = pm.find_MAP(model=weibull_2seg_2clust_withspike, fmin=so.fmin_powell)\n",
    "# print(map_estimate)\n",
    "\n",
    "## Specifying start, sampler (step) and multiple chains\n",
    "# with weibull_2seg_2clust_withspike:\n",
    "#     start = pm.find_MAP()\n",
    "#     step = pm.Metropolis()\n",
    "#     start_step_trace = pm.sample(1000, step=step, start=start, njobs=4, random_seed=123)\n",
    "\n",
    "## Merging Traces. TODO: This has some issue. Fix later\n",
    "# from pymc3.backends.base import merge_traces\n",
    "# with weibull_2seg_2clust_withspike:\n",
    "#     merg_trace = merge_traces([pm.sample(1000, step, chain_idx=i) for i in range(4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze sampling results\n",
    "\n",
    "# db = pm.backends.Text('test')\n",
    "# trace = pm.sample(..., trace=db)\n",
    "\n",
    "tmp_trace = trace\n",
    "\n",
    "print(tmp_trace.nchains)\n",
    "pm.traceplot(tmp_trace), plt.show()\n",
    "display(pm.summary(tmp_trace))\n",
    "# display(pm.summary(tmp_trace[500:], varnames=['shape']))\n",
    "pm.forestplot(tmp_trace), plt.show()\n",
    "print(pm.effective_n(tmp_trace[500:]))\n",
    "pm.plot_posterior(trace), plt.show()\n",
    "# pm.autocorrplot(tmp_trace[500:], varnames=['shape']), plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinker with Weibull and Weibull Gamma Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def weib(x, scale, shape):\n",
    "    return (shape/scale) * (x/scale)**(shape-1) * np.exp(-(x/scale)**shape)\n",
    "\n",
    "g_shape, g_scale = 5., 2. #3., 0.5152451259005756\n",
    "\n",
    "data_x = np.arange(1,200.)/50.\n",
    "data_prob = weib(data_x, g_scale, g_shape) #data, scale, shape\n",
    "plt.plot(data_x, data_prob)\n",
    "\n",
    "plt.figure()\n",
    "raw_values = pm.Weibull.dist(alpha=g_shape, beta=g_scale).random(size=1000) #alpha:shape, beta:scale\n",
    "# raw_values = 2*np.random.weibull(5.,1000) #specify shape, scale is fixed at 1\n",
    "count, bins, ignored = plt.hist(raw_values, bins=10)\n",
    " \n",
    "scale = count.max()/data_prob.max()\n",
    "print(count.max(), data_prob.max(), scale)\n",
    "plt.plot(data_x, data_prob*scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weibull function fit using scipy optimizer\n",
    "\n",
    "# https://github.com/scipy/scipy/blob/master/scipy/optimize/_minimize.py\n",
    "# http://blog.mmast.net/optimization-scipy\n",
    "\n",
    "from scipy.optimize import minimize, fmin\n",
    "\n",
    "def weib(x, scale, shape):\n",
    "    return (shape/scale) * (x/scale)**(shape-1) * np.exp(-(x/scale)**shape)\n",
    "\n",
    "# def custom_weibull(x, lamb, c, max_len):\n",
    "#     cdf = lambda x, lamb, c: 1.0 - np.exp(-lamb * np.float_power(x, c))\n",
    "#     if x == 1:\n",
    "#         return cdf(x, lamb, c)\n",
    "#     if x == max_len:\n",
    "#         return 1 - cdf(x-1, lamb, c)\n",
    "#     return cdf(x, lamb, c) - cdf(x-1, lamb, c)\n",
    "\n",
    "# def weibull_neg_loglike(params, tickets, half=False):\n",
    "#     prob = lambda w, x, lamb, c, max_len: -w * np.log(custom_weibull(x, lamb, c, max_len))\n",
    "#     # weibull requires x > 0; so, send i + 1 instead of i\n",
    "#     return np.sum([prob(tickets[i], i + 1, params[0], params[1], len(tickets)) for i in range(len(tickets))])\n",
    "\n",
    "bin_width = 50.\n",
    "data_x = np.arange(1,200.)/bin_width\n",
    "data_prob = weib(data_x, 2., 5.) #data, scale, shape\n",
    "\n",
    "def weib_neg_loglike_ver1(theta):\n",
    "    prob = lambda w, x, scale, shape: -w * np.log( (shape/scale) * (x/scale)**(shape-1) * np.exp(-(x/scale)**shape) )\n",
    "    return np.sum([prob(data_prob[i], i+1, theta[0], theta[1]) for i in range(len(data_prob))])\n",
    "\n",
    "# https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=3508&context=etd\n",
    "def weib_neg_loglike_ver2(theta): #Giving buggy results fix later\n",
    "    n, scale, shape = len(data_prob), theta[0], theta[1]\n",
    "    sum_1, sum_2 = 0., 0.\n",
    "    for i in range(1,len(data_prob)+1):\n",
    "        sum_1 = sum_1 + np.log(data_prob[i-1])\n",
    "        sum_2 = sum_2 + (data_prob[i-1]/scale)**shape\n",
    "    return -1* ( n*np.log(shape) - shape*n*np.log(scale) + (shape-1)*sum_1 - sum_2 )\n",
    "\n",
    "theta_start = [100., 10.] #scale, shape\n",
    "res = minimize(weib_neg_loglike_ver1, theta_start, method='Nelder-Mead', options={'disp':True})\n",
    "# res = fmin(weib_neg_loglike_ver1, theta_start, xtol = 0.0000001, ftol = 0.0000001, disp = 1)\n",
    "print(res)\n",
    "print('Shape:{}|Scale:{}|RShape:{}|RScale:{}'.format(res.x[1],(res.x[0]/bin_width),round(res.x[1]),round(res.x[0]/bin_width))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Weibull Gamma optimization on simulated data\n",
    "\n",
    "# F(t|lambda,c) = 1 - e^(-lambda*t^c)\n",
    "# g(lambda|r,alpha) = alpha^r*lambda^(r-1)*e^(-alpha*lambda) / Gamma(r)\n",
    "# https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.gamma.html\n",
    "\n",
    "# def weib(x, scale, shape):\n",
    "#     return (shape/scale) * (x/scale)**(shape-1) * np.exp(-(x/scale)**shape)\n",
    "\n",
    "simulated_data = np.array([])\n",
    "w_shape, g_r, g_alpha =  3., 10., 1./0.9 #w-shape, g-shape, g-rate\n",
    "lamba_arr = np.random.gamma(shape=g_r, scale=1.0/g_alpha, size=1)\n",
    "for lambda_v in lamba_arr:\n",
    "    w_scale = np.exp(-np.log(lambda_v)/w_shape)\n",
    "    bin_width = 50.\n",
    "    data_x = np.arange(1,200.)/bin_width\n",
    "    data_prob = weib(data_x, w_scale, w_shape) #data, scale, shape\n",
    "    #print(lambda_v, w_scale, count.max(), data_prob.max(), tmp_scale)\n",
    "    plt.plot(data_x, data_prob)\n",
    "    #simulated_data = np.append(simulated_data, pm.Weibull.dist(alpha=w_shape, beta=w_scale).random(size=1000))\n",
    "    simulated_data = data_prob\n",
    "\n",
    "def weib_gamma_neg_loglike(theta):\n",
    "    def prob(w, x, w_shape, g_shape, g_scale):\n",
    "        w_scale_var = ss.gamma(g_shape, scale=g_scale)\n",
    "        w_scale = w_scale_var.mean()\n",
    "        res = -w * np.log( (w_shape/w_scale) * (x/w_scale)**(w_shape-1) * np.exp(-(x/w_scale)**w_shape) )\n",
    "        return res\n",
    "    return np.sum([prob(simulated_data[i], i+1, theta[0], theta[1], theta[2]) for i in range(len(simulated_data))])\n",
    "\n",
    "theta_start = [4., 11., 12.5] #w-shape, g-shape, g-scale\n",
    "res = minimize(weib_gamma_neg_loglike, theta_start, method='Nelder-Mead', options={'disp':True}) #method='Nelder-Mead', \n",
    "print(res)\n",
    "\n",
    "#true param : [3., 10., 0.9] #w-shape, g-shape, g-rate\n",
    "#start:[4., 11., 12.5], end:[2.07498354, 10.5404605 , 12.09560527], method:default (bfgs ...)\n",
    "#start:[4., 11., 12.5], end:[2.95151024, 4.70356652, 4.50075516], method:Nelder-Mead\n",
    "#start:[5., 12., 5.], end:[4.33672378, 11.70704407,  4.29690579], method:default (bfgs ...)\n",
    "#start:[5., 12., 5.], end:[3.00004045, 15.72523612,  1.46519853], method:Nelder-Mead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize mixture of weibull cdf\n",
    "\n",
    "def weibull_2seg_2clust_withspike_negloglike(theta):\n",
    "    prob = lambda x, y, z: -x * np.log(gamma.pdf(y, z))\n",
    "    data = [10, 9, 8, 7, 6, 5]\n",
    "    return np.sum([prob(data[i], i+1, theta) for i in range(len(data))])\n",
    "\n",
    "\n",
    "params = fit_weibull_2seg_2clust_withspike(params, [clust1_ts, clust2_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scipy stats exponweib distribution fit to dummy data\n",
    "\n",
    "data = [10,9,8,7,6,5,4,4,4,5,6,7,7,7]\n",
    "plt.plot(data, ss.exponweib.pdf(data, *ss.exponweib.fit(data, 1, 1, scale=2, loc=0)))\n",
    "_ = plt.hist(data, bins=np.linspace(0, 16, 33), normed=True, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##PyMC distribution\n",
    "\n",
    "# y = pm.Weibull.dist(alpha=20, beta=1000) #shape, scale\n",
    "# print(y.logp(1000).eval())\n",
    "# sns.distplot(y.random(size=1000))\n",
    "\n",
    "# y1 = pm.Weibull.dist(alpha=0.9, beta=10) \n",
    "# y2 = pm.Weibull.dist(alpha=20, beta=1000)\n",
    "# # y3 = pm.Weibull.dist(alpha=1, beta=500)\n",
    "# raw_values = np.append(y1.random(size=1000), y2.random(size=1000))\n",
    "# sns.distplot(raw_values)\n",
    "# plt.figure(), plt.hist(raw_values)\n",
    "\n",
    "\n",
    "y1 = pm.Weibull.dist(alpha=2, beta=10) \n",
    "raw_values = y1.random(size=1000)\n",
    "sns.distplot(raw_values)\n",
    "plt.figure(), plt.hist(raw_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Gamma and Gamma Gamma Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.brucehardie.com/notes/025/gamma_gamma.pdf\n",
    "\n",
    "import scipy.special as sps\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# shape, scale = 1.55, 3.9  # mean=4, std=2*sqrt(2)\n",
    "# s = np.random.gamma(shape, scale, 1000)\n",
    "# count, bins, ignored = plt.hist(s, 50, normed=True)\n",
    "# y = bins**(shape-1)*(np.exp(-bins/scale) / (sps.gamma(shape)*scale**shape))\n",
    "# plt.plot(bins, y, linewidth=2, color='r')\n",
    "# plt.show()\n",
    "\n",
    "## Plot Gamma-Gamma function\n",
    "p, q, g = 2.8, 14.8, 441.7\n",
    "nu = np.random.gamma(q, 1.0/g, 10)\n",
    "for nu_val in nu:\n",
    "    s = np.random.gamma(p, 1.0/nu_val, 1000)\n",
    "    shape, scale = p, 1.0/nu_val \n",
    "    count, bins, ignored = plt.hist(s, 50, normed=True)\n",
    "    y = bins**(shape-1)*(np.exp(-bins/scale) / (sps.gamma(shape)*scale**shape))\n",
    "    plt.plot(bins, y, linewidth=2, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma Gamma Regression (tinker with tensorflow or  pytorch or scipy distr or statsmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list_list = [['a','b',5,6,7,8,7,6,5,4,3,2,1,1],['c','d',6,6,7,7,8,9,6,3,3,3,1,1],['a','b',7,8,9,10,9,8,7,6,5,4,3,3],['c','d',9,9,10,10,11,12,9,6,6,6,4,4]] \n",
    "tmp_df = pd.DataFrame(tmp_list_list) \n",
    "tmp_df.columns = list(map(lambda x : 'c'+str(x), tmp_df.columns))\n",
    "\n",
    "freq = np.array(tmp_df.iloc[0][2:].values.tolist())\n",
    "val = np.array(list(map(lambda x : int(x[1:]), tmp_df.columns[2:].tolist())))\n",
    "\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lifetimes import GammaGammaFitter\n",
    "\n",
    "ggf = GammaGammaFitter(penalizer_coef = 0)\n",
    "ggf.fit(freq, val)\n",
    "print(ggf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson, Exponential Regression (tensorflow, pytorch, scipy distr, statsmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinker with Beta Binomial Distribution (PyMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame([['a',10,5],['b',10,2],['c',12,2],['d',20,5],['e',100,1]], columns=['scenario','sample_size','conversions'])\n",
    "tmp_df['cr'] = 1.0 * tmp_df['conversions'] / tmp_df['sample_size']\n",
    "num_samples = len(tmp_df)\n",
    "\n",
    "print(tmp_df.shape)\n",
    "display(tmp_df['conversions'].describe())\n",
    "alpha1, beta1, loc1, scale1 = ss.beta.fit(tmp_df['cr'][0:500])\n",
    "print(alpha1, beta1, loc1, scale1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://docs.pymc.io/notebooks/hierarchical_partial_pooling.html\n",
    "# https://stackoverflow.com/questions/43710346/difference-between-betabinomial-and-beta-and-binomial\n",
    "\n",
    "sample_size_arr, obs_arr = tmp_df['sample_size'].values, tmp_df['conversions'].values\n",
    "\n",
    "with pm.Model() as bb:\n",
    "    scenarios = pm.Beta('scenarios', alpha=alpha1, beta=beta1, shape=num_samples)\n",
    "    cr_like = pm.Binomial('cr_like', n=sample_size_arr, p=scenarios, observed=obs_arr)\n",
    "\n",
    "    #alpha0 = pm.Uniform('alpha', 1, 100)\n",
    "    #beta0 = pm.Uniform('beta', 1, 100)\n",
    "    #cr_like = pm.BetaBinomial('cr_like', alpha=alpha0, beta=beta0, n=sample_size_arr, observed=obs_arr)\n",
    "\n",
    "with bb:\n",
    "    hierarchical_trace = pm.sample(draws=1000)#, n_init=1000)\n",
    "\n",
    "tmp_trace = hierarchical_trace\n",
    "summary_df = pm.summary(tmp_trace[500:])\n",
    "summary_df['mean'].describe(), tmp_df['cr'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
