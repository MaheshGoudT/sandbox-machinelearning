{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import sklearn.preprocessing\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy data for regression\n",
    "\n",
    "tmp_df = pd.DataFrame([['a',1,2],['b',3,4],['a',5,6],['b',7,8],['a',9,10],['b',11,12]])\n",
    "tmp_df.columns = ['c1','c2','c3']\n",
    "categorical_features = ['c1','c2']\n",
    "X_train = X_test = pd.get_dummies(tmp_df[categorical_features])\n",
    "Y_train = Y_test = tmp_df['c3']\n",
    "\n",
    "enc_dict = defaultdict(sklearn.preprocessing.LabelEncoder)\n",
    "ohe = sklearn.preprocessing.OneHotEncoder()\n",
    "tmp_le_df = tmp_df[categorical_features].apply(lambda x : enc_dict[x.name].fit_transform(x))\n",
    "ohe.fit(tmp_le_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.statsmodels.org/stable/glm.html\n",
    "# https://www.statsmodels.org/stable/example_formulas.html\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.scotland.load()\n",
    "data.exog = sm.add_constant(data.exog)\n",
    "gamma_model = sm.GLM(data.endog, data.exog, family=sm.families.Gamma())\n",
    "gamma_results = gamma_model.fit()\n",
    "print(gamma_results.summary())\n",
    "\n",
    "# formula = 'y ~ x1 + x2 + x3'\n",
    "# mdl = sm.GLM(y, X, family=sm.families.Binomial)\n",
    "# res = mdl.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample specific uncertainty intervals for regression models\n",
    "\n",
    "# clf = sklearn.linear_model.LinearRegression()\n",
    "# clf.fit(X_train, Y_train)\n",
    "# Y_predictions = clf.predict(X_test)\n",
    "# print(clf.coef_, clf.intercept_)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# ##Train/Test prediction intervals using statsmodel. Use bootstrap or below approaches\n",
    "# #Theory: https://onlinecourses.science.psu.edu/stat414/node/298/\n",
    "# #http://nbviewer.jupyter.org/gist/thatneat/10286720\n",
    "# #https://www.learndatasci.com/tutorials/predicting-housing-prices-linear-regression-using-python-pandas-statsmodels/\n",
    "# #https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html\n",
    "# #https://stats.stackexchange.com/questions/183230/bootstrapping-confidence-interval-from-a-regression-prediction\n",
    "\n",
    "# Using stats model\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "# re = sm.OLS(Y_train, X_train).fit()\n",
    "# print(re.summary())\n",
    "# print('-'*80)\n",
    "# prstd, iv_l, iv_u = wls_prediction_std(re)\n",
    "# print(prstd, iv_l, iv_u)\n",
    "# print('-'*80)\n",
    "# prstd, iv_l, iv_u = wls_prediction_std(re, X_train.iloc[0:2]) #Test set can be invoked through this approach\n",
    "# print(prstd, iv_l, iv_u)\n",
    "\n",
    "# Using bootstrap \n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "n_estimators = 50\n",
    "model = BaggingRegressor(sklearn.linear_model.LinearRegression(), n_estimators=n_estimators, bootstrap=True)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "#Inspecting uncertainty interval for a prediction at sample test point\n",
    "test_sample = np.array([[1,2,3]])\n",
    "res = []\n",
    "for m in model.estimators_:\n",
    "    res.append(m.predict(test_sample))\n",
    "pd.DataFrame(res).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding based Neural Network Regressor. Check out https://www.kaggle.com/aquatic/entity-embedding-neural-net\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers import Input, Embedding, merge, Reshape, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "categorical_features = ['c1','c2']\n",
    "numeric_features = []\n",
    "predict_col_name = 'c3'\n",
    "all_cols = categorical_features\n",
    "\n",
    "## Reformatting train/test data for keras model\n",
    "\n",
    "tmp_X_train, tmp_X_test = tmp_df, tmp_df\n",
    "tmp_le_train_df = tmp_X_train[categorical_features].apply(lambda x: enc_dict[x.name].transform(x))\n",
    "tmp_le_test_df = tmp_X_test[categorical_features].apply(lambda x: enc_dict[x.name].transform(x))\n",
    "\n",
    "tmp_X_train_inp = []\n",
    "tmp_X_test_inp = []\n",
    "for c in all_cols:\n",
    "    tmp_X_train_inp.append(np.array(tmp_le_train_df[c]))\n",
    "    tmp_X_test_inp.append(np.array(tmp_le_test_df[c]))\n",
    "\n",
    "print('Train test shape:', tmp_le_train_df.shape, tmp_le_test_df.shape)\n",
    "tmp_num_feat = {}\n",
    "for col in categorical_features:\n",
    "    num_uniq_features = len(enc_dict[col].classes_)\n",
    "    print (col, num_uniq_features)\n",
    "    tmp_num_feat.update({col:num_uniq_features})  # Feature embedding based regression model using keras\n",
    "\n",
    "embedding_size = 5\n",
    "batch_size = 16\n",
    "\n",
    "# # early_stopping = EarlyStopping(monitor='val_loss', patience=0)\n",
    "# tmp_model_save_dir = '/Users/maheshgoud/Desktop/'\n",
    "# tmp_model_save_filename = tmp_model_save_dir + 'model_tmp.h5'\n",
    "\n",
    "## Keras model implementation\n",
    "\n",
    "tmp_input_list = []\n",
    "tmp_embedding_list = []\n",
    "for idx, e_col in enumerate(all_cols):\n",
    "    tmp_input = Input(shape=(1,), dtype='int32', name=e_col)\n",
    "    tmp_embedding = Embedding(input_dim=tmp_num_feat[e_col], output_dim=embedding_size, input_length=1)(tmp_input)\n",
    "    tmp_input_list.append(tmp_input)\n",
    "    tmp_embedding_list.append(tmp_embedding)\n",
    "\n",
    "x = layers.concatenate(tmp_embedding_list)\n",
    "x = Reshape((len(tmp_embedding_list)*embedding_size,), name=\"reshape_one\")(x)\n",
    "x = Dense(64, activation='relu', name=\"dense_1\")(x)\n",
    "x = Dropout(.1)(x)\n",
    "x = Dense(32, activation='relu', name=\"dense_2\")(x)\n",
    "tmp_model_output = Dense(1, activation='relu', name=\"dense_3\")(x)\n",
    "\n",
    "tmp_final_model = Model(input=tmp_input_list, output=tmp_model_output)\n",
    "print(tmp_final_model.summary())\n",
    "\n",
    "# mean_squared_error, mean_absolute_percentage_error, mean_squared_logarithmic_error\n",
    "tmp_final_model.compile(loss='mean_absolute_percentage_error', optimizer='adadelta') #metrics=['accuracy']\n",
    "tmp_final_model.fit(tmp_X_train_inp, Y_train.values, epochs=5, batch_size=batch_size, validation_split=0.1) #, callbacks=[early_stopping])\n",
    "\n",
    "# tmp_final_model.save(tmp_model_save_filename)\n",
    "\n",
    "Y_predictions = tmp_final_model.predict(tmp_X_test_inp, batch_size=batch_size)\n",
    "mean_absolute_percentage_error(Y_test.values, Y_predictions) #TODO: Double check\n",
    "\n",
    "# from ann_visualizer.visualize import ann_viz\n",
    "# ann_viz(tmp_final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy data for classification\n",
    "\n",
    "tmp_df = pd.DataFrame([['a',1,0],['b',3,1],['a',5,0],['b',7,1],['a',9,0],['b',11,1]])\n",
    "tmp_df.columns = ['c1','c2','c3']\n",
    "categorical_features = ['c1','c2']\n",
    "\n",
    "X_train = X_test = pd.get_dummies(tmp_df[categorical_features])\n",
    "Y_train = Y_test = tmp_df['c3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation on single train/test split for inspecting confusion matrix\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegression(C=1e4)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_predictions = clf.predict(X_test)\n",
    "\n",
    "#Evaluate predictions\n",
    "score = 100.0*round(clf.score(X_test, Y_test),4)\n",
    "print('Accuracy:',score)\n",
    "print(sklearn.metrics.classification_report(Y_test, Y_predictions, target_names=['(0)','(1)']))\n",
    "cm = sklearn.metrics.confusion_matrix(Y_test, Y_predictions)\n",
    "print('ConfusionMatrix\\n',cm)\n",
    "\n",
    "#Color coded confusion matrix\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(title, size = 15)\n",
    "plt.show()\n",
    "\n",
    "#scikitplot can be used for roc curves. skipping for now\n",
    "\n",
    "# #Precision/Recall curve\n",
    "# y_scores = sklearn.model_selection.cross_val_predict(clf, X_train, Y_train, cv=3, method=\"decision_function\")\n",
    "# precisions, recalls, thresholds = sklearn.metrics.precision_recall_curve(Y_train, y_scores)\n",
    "\n",
    "#Inspect model coefficients\n",
    "print('Model coefficients:')\n",
    "pd.DataFrame({'feat':X_train.columns,'coef':clf.coef_[0]}).sort_values(by=['coef'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical embedding + numerical data neural network classifier\n",
    "\n",
    "#Embedding based neural network\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, merge, Reshape, Dropout, Input, Flatten, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_embeddingnetwork(df, categorical_cols, numeric_cols, timestamp_cols=None): \n",
    "    inputs, embeddings = [], []\n",
    "    for c in categorical_cols:\n",
    "        #Determined embedding dimension\n",
    "        no_of_unique_cat  = df[c].nunique()\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "        embedding_size = int(embedding_size)\n",
    "        #Create embedding\n",
    "        cur_input = Input(shape=(1,), name=c)\n",
    "        cur_embedding = Embedding(no_of_unique_cat+1, embedding_size, input_length=1)(cur_input)\n",
    "        cur_embedding = Reshape(target_shape=(embedding_size,))(cur_embedding)\n",
    "        inputs.append(cur_input)\n",
    "        embeddings.append(cur_embedding)\n",
    "    input_numeric = Input(shape=(len(numeric_cols),))\n",
    "    embedding_numeric = Dense(16)(input_numeric) \n",
    "    inputs.append(input_numeric)\n",
    "    embeddings.append(embedding_numeric)\n",
    "\n",
    "    x = Concatenate()(embeddings)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(.35)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(.15)(x)\n",
    "    x = Dense(8, activation='relu')(x)\n",
    "    x = Dropout(.15)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "#Create train/test lists\n",
    "embed_and_num_train_list, embed_and_num_test_list = [], []\n",
    "for c in cat_feat:\n",
    "    embed_and_num_train_list.append(X_train_without_ohe[c].values)\n",
    "    embed_and_num_test_list.append(X_test_without_ohe[c].values)\n",
    "embed_and_num_train_list.append(X_train_without_ohe[num_feat].values)\n",
    "embed_and_num_test_list.append(X_test_without_ohe[num_feat].values)\n",
    "\n",
    "model = build_embeddingnetwork(X_train_without_ohe, cat_feat, num_feat)\n",
    "print(model.summary())\n",
    "# model.fit(X_train_without_ohe[cat_feat+num_feat], Y_train.values, epochs=50, batch_size=1024, validation_split=0.1, verbose=0)\n",
    "# Y_predictions = model.predict(X_test_without_ohe[cat_feat+num_feat], batch_size=batch_size)\n",
    "model.fit(embed_and_num_train_list, Y_train.values, epochs=10, batch_size=1024, validation_split=0.1, verbose=0)\n",
    "Y_predictions = model.predict(embed_and_num_test_list, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow lr what if ? tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poisson, exp regression with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch mean squared error regression\n",
    "\n",
    "import torch \n",
    "\n",
    "x1_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y1_data = torch.Tensor([[10.0], [20.0], [30.0]]) #y1=10*x1\n",
    "# x1_data = torch.Tensor([[1.0,2.0], [2.0,3.0], [3.0,4.0]])\n",
    "# y1_data = torch.Tensor([[3.0], [5.0], [7.0]]) #y1=sum(x1)\n",
    "\n",
    "class LinearRegressionModel(torch.nn.Module): \n",
    "    def __init__(self, input_dim, output_dim): \n",
    "        super(LinearRegressionModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x): \n",
    "        y_pred = self.linear(x) \n",
    "        return y_pred \n",
    "\n",
    "our_model = LinearRegressionModel(1,1) \n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(our_model.parameters(), lr = 0.01) \n",
    "\n",
    "for epoch in range(5000): \n",
    "    pred_y = our_model(x1_data) \n",
    "    loss = criterion(pred_y, y1_data) \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    #print('epoch {}, loss {}'.format(epoch, loss.data)) \n",
    "print(our_model.state_dict())\n",
    " \n",
    "new_var = torch.Tensor([[4.0]])\n",
    "pred_y = our_model(new_var)\n",
    "print(\"predict (after training)\", our_model(new_var).data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PyTorch MSE multi task loss \n",
    "# https://discuss.pytorch.org/t/multi-task-learning-with-adaptive-weights-for-task-losses/24961\n",
    "# https://github.com/huggingface/hmtl\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# x2_data = torch.Tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "# y2_data = torch.Tensor([[10, 3.0], [20, 5.0], [30, 7.0]]) #y2=[10*x2[0], sum(x2)]\n",
    "x2_data = torch.Tensor([[1.2,2.3],[1.6,2.3],[1.9,2.2],[1.4,2.5],[1.3,2.9],[1.4,-2.5],[1.3,-2.9]])\n",
    "y2_data = torch.Tensor([[12,3.5],[16,3.9],[19,4.1],[14,3.9],[13,4.2],[14,-0.1],[13,-1.6]])\n",
    "\n",
    "class MultiTaskModel(nn.Module): \n",
    "    def __init__(self, input_dim, output_dim): \n",
    "        super(MultiTaskModel, self).__init__() \n",
    "        self.task1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1, bias=False),\n",
    "        )\n",
    "        self.task2 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1, bias=False),\n",
    "        )\n",
    "        #self.task1 = self.task2\n",
    "    def forward(self, x): \n",
    "        return self.task1(x), self.task2(x) \n",
    "\n",
    "mt_model = MultiTaskModel(2,1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(mt_model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(10000): \n",
    "    predictions = mt_model(x2_data) \n",
    "    loss = criterion(predictions[0], y2_data[:,0]) #+ criterion(predictions[1], y2_data[:,1]) \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    #print('epoch {}, loss {}'.format(epoch, loss.data)) \n",
    "print(\"Model params:\",mt_model.state_dict())\n",
    "\n",
    "new_var = torch.Tensor([[4.0, 5.0]])\n",
    "print(\"Prediction on a test point:\", mt_model(x2_data))\n",
    "print(\"Testing without bias:\", torch.mm(mt_model.state_dict()['task2.0.weight'], new_var.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tinkering with hyperopt\n",
    "# https://www.kaggle.com/dreeux/hyperparameter-tuning-using-hyperopt\n",
    "# https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/\n",
    "# https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "import hyperopt.pyll.stochastic\n",
    "\n",
    "space = {\n",
    "        'max_depth': hp.choice('max_depth', np.arange(5, 25, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child', 1, 10, 1),\n",
    "        'subsample': hp.uniform('subsample', 0.8, 1),\n",
    "        'n_estimators' : hp.choice('n_estimators', np.arange(100, 500, 100, dtype=int)),\n",
    "        'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n",
    "        'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'scale_pos_weight' : hp.choice('scale_pos_weight',[1,2,5,10,50,100,1000])\n",
    "    }\n",
    "\n",
    "print (hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
